---
title: "Computational Problem"
author: "Yuhuan Huang"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r, message=FALSE, warning=FALSE}
#getwd()
#setwd("./Documents/R-for-Econometrics")

library(haven)
library(dplyr)
library(broom)
library(ggplot2)
library(segmented)
data <- read_dta("lalonde2.dta")
```

\section{Q1}

```{r}
covariates <- c("age", "educ", "black", "married", "nodegree", "hisp", "kids18", "kidmiss", "re74")

data <- data %>%
  mutate(group = case_when(
    treated == 1 & sample == 1 ~ "Treated_NSW",
    treated == 0 & sample == 1 ~ "Compar_NSW",
    sample == 2 ~ "Compar_CPS",
    TRUE ~ "Other"
  ))

table <- data %>%
  group_by(group) %>%
  filter(group != "Other") %>%
  summarise(across(all_of(covariates), list(
    mean = \(x) mean(x, na.rm = TRUE),
    sd = \(x) sd(x, na.rm = TRUE)
  )))
print(table)
```

The result(table) is shown above. Compare treated NSW group with comparison NSW group, we can see that the metrics(mean and standard deviation) are very similar, while the treated group has a slightly higher mean in those variables. Compare treated NSW group with comparison CPS group, we can see that the groups have relatively large difference! The comparison CPS data has a larger average age and larger average education, etc.

\section{Q2}


```{r}
data_filtered <- data %>%
  filter(group %in% c("Treated_NSW", "Compar_CPS"))

data_filtered$treated[is.na(data_filtered$treated)] <- 0 ## CPS data "treated" should be zero.

model_no_cov <- lm(re78 ~ 1 + treated, data = data_filtered)  
model_with_cov <- lm(re78 ~ 1 + treated + age + educ + black + married + nodegree +
                       + hisp + kids18 + kidmiss + re74
                       ,data = data_filtered)

summary(model_no_cov)
summary(model_with_cov)
```


It is very different from PS1's result. When using the data made up of the treated group from NSW data and the comparison data from the CPS group, the coefficient of the variable "treated" is significantly different from what we get from the regression using both treated and non-treated NSW data. Also we can see that the difference between the with covariance model and without covariance model is different from what we get in PS1. A possible explanation of this is that, the ones from the CPS is not a good comparison group of the treated ones in NSW. This also reflects what we get from Q1: That the other dimensions of the treated group and its comparison (other variables such as age, educ, and black) are very different.

\section{Q3}

```{r}
data_cps <- data %>%
  filter(group == "Compar_CPS") 


ggplot(data_cps, aes(x = educ, y = re78)) +
  geom_smooth(method = "lm", color = "red", se = TRUE) +  # Regression line with confidence interval
  theme_minimal() +
  labs(title = "Education and Real Earnings",
       x = "Years of Education",
       y = "Real Earnings in 1978")

ggplot(data_cps, aes(x = factor(educ), y = re78)) +  # 
  geom_boxplot(outlier.shape = NA, fill = "#69b3a2", alpha = 0.5) + 
  theme_minimal() +
  labs(title = "Education and Real Earnings",
       x = "Years of Education",
       y = "Real Earnings in 1978")
```

From the regression plot and the box plot, we can see that, the real earnings in 1978 is positively correlated with years of education. And according to the box plot, there is a comparatively significant difference between education years of 12-18, 3-11, and 1-2.



\section{Q4}

```{r}
# Create piecewise terms
data_cps$edu_above12 <- pmax(0, data_cps$educ - 12)
data_cps$edu_above16 <- pmax(0, data_cps$educ - 16)

model <- lm(re78 ~ educ + edu_above12 + edu_above16, data = data_cps)
summary(model)

# Plot the results
# Get model coefficients
coeffs <- coef(model)
intercept <- coeffs[1]
slope1 <- coeffs[2]  # Slope before 12 years
slope2 <- slope1 + coeffs[3]  # Slope between 12 and 16 years
slope3 <- slope2 + coeffs[4]  # Slope after 16 years

cat("slope1 =", slope1, ", slope2 =", slope2, " ,slope3 = ", slope3, "\n")
```

We set two dummies: education years above 12 and education years above 16. The result of the model means that: The slope of the first regression is the the coefficient of educ plus the coefficient of edu_above12, which is 509.97; the slope of the second regression is the coefficient of educ plus the coefficient of edu_above12 plus the coefficient of edu_above16, which is 484.66; the slope of the third regression is the coefficient of educ plus the coefficient of edu_above12 plus the coefficient of educ_above16, which is -221.72. It means that, when you have education less than 12 years, keep other things the same, on average 1 year's increase in education would bring about 509 dollars in salary; when you have years of education between 12 to 16, keep other things the same, on average 1 year's increase in education would bring about 484.66 salary increase; But if you already have 16 years and above (have a bachelor's degree already), keep other things the same, an increase in a year of education would decrease the average salary by about 221.72 dollars.

We also draw the graph below. Since the slope of the first and the second segment are close, it is not easy to see the two regression lines on the graph.

```{r}
###### Plot
educ_seq <- seq(min(data_cps$educ), max(data_cps$educ), by = 0.1)

# Create a data frame for segmented predictions
pred_df <- data.frame(
  educ = educ_seq,
  segment = factor(
    case_when(
      educ_seq <= 12 ~ "Before 12 years",
      educ_seq > 12 & educ_seq <= 16 ~ "Between 12-16 years",
      educ_seq > 16 ~ "After 16 years"
    ),
    levels = c("Before 12 years", "Between 12-16 years", "After 16 years")
  ),
  re78 = case_when(
    educ_seq <= 12 ~ intercept + slope1 * educ_seq,
    educ_seq > 12 & educ_seq <= 16 ~ (intercept + slope1 * 12) + slope2 * (educ_seq - 12),
    educ_seq > 16 ~ (intercept + slope1 * 12 + slope2 * 4) + slope3 * (educ_seq - 16)
  )
)

# Plot the segmented piecewise regression with different colors
ggplot(data_cps, aes(x = educ, y = re78)) +
  geom_point(alpha = 0.5, color="grey") +  # Scatter plot of data points
  geom_line(data = pred_df, aes(x = educ, y = re78, color = segment), size = 1.2) +  # Color-coded piecewise line
  geom_vline(xintercept = c(12, 16), linetype = "dashed", color = "black", size = 1) +  # Threshold markers
  scale_color_manual(values = c("Before 12 years" = "red", 
                                "Between 12-16 years" = "blue", 
                                "After 16 years" = "green")) +  # Custom segment colors
  labs(title = "Piecewise Linear Regression: re78 vs. educ",
       x = "Years of Education (educ)",
       y = "Earnings in 1978 (re78)",
       color = "Education Range") +
  theme_minimal()
```


I also used package "segmented" to automatically select breakpoints. The breakpoints selected are 11 and 12. The automatic model indeed has a slightly higher R-square than the model using breakpoints 12 and 16.


```{r}
########## Use Package "Segmented": automatic choose breakpoints


base_model <- lm(re78 ~ educ, data = data_cps)
seg_model <- segmented(base_model, seg.Z = ~educ, psi = c(12, 16))
summary(seg_model)

data_cps$predicted <- predict(seg_model)
breakpoints <- seg_model$psi[, "Est."]

ggplot(data_cps, aes(x = educ, y = re78)) +
  geom_point(alpha = 0.5, color = "grey") +  
  geom_line(aes(y = predicted), color = "red", size = 1.2) +  
  geom_vline(xintercept = breakpoints, linetype = "dashed", color = "blue") + 
  labs(title = "Segmented Regression: re78 vs. educ",
       x = "Years of Education (educ)",
       y = "Earnings in 1978 (re78)") +
  theme_minimal()
```


